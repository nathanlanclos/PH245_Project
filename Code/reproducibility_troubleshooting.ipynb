{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Troubleshooting Model Performance**\n",
    "\n",
    "In this notebook I sought to rerun the models that performed the best in Dec. 2024 and cleaned up some of the useless cells. The goal here was to replicate the results we reported, double check the code worked, and see if I could help troubleshoot new modeling attempts in June '25\n",
    "\n",
    "Nathan Lanclos\n",
    "06/19/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Processing**\n",
    "\n",
    "In the first couple of cells, we are just processing the data into train and test sets and scaling the data for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Path to data folder\n",
    "directory = '../GATSol/dataset/'\n",
    "\n",
    "# Load CSVs\n",
    "df_train = pd.read_pickle(directory + 'eSol_train.pkl')\n",
    "df_test = pd.read_pickle(directory + 'eSol_test.pkl')\n",
    "\n",
    "continuous_column = 'solubility' \n",
    "binary_column = 'binary_solubility'\n",
    "\n",
    "# Identify scalar features\n",
    "scalar_features = [\n",
    "    col for col in df_train.columns\n",
    "    if pd.api.types.is_numeric_dtype(df_train[col]) and col != continuous_column and col != binary_column\n",
    "]\n",
    "\n",
    "# Extract scalar features and target for regression and classification\n",
    "X_train = df_train[scalar_features]\n",
    "y_train_continuous = df_train[continuous_column]\n",
    "y_train_binary = df_train[binary_column]\n",
    "X_test = df_test[scalar_features]\n",
    "y_test_continuous = df_test[continuous_column]\n",
    "y_test_binary = df_test[binary_column]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2019, 67)\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "print(X_train_scaled_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model below, we are simply fitting a linear regression with the scalar features from sequence and structure feature engines, no embeddings are included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Linear Regression ===\n",
      "\n",
      "Sorted Coefficients (Linear Regression):\n",
      "                               Feature  Coefficient\n",
      "45                           num_atoms     3.754350\n",
      "46                          total_mass    -2.707929\n",
      "0                     molecular_weight    -1.541898\n",
      "61              O_atom_type_proportion     0.725713\n",
      "59              N_atom_type_proportion    -0.611208\n",
      "..                                 ...          ...\n",
      "15        Hydrophobicity_FASG890101-G2    -0.004357\n",
      "55                       b_factors_min     0.003076\n",
      "50                 bounding_box_volume     0.001675\n",
      "26  Normalized van der Waals Volume-G1    -0.000721\n",
      "48                          num_chains     0.000000\n",
      "\n",
      "[67 rows x 2 columns]\n",
      "\n",
      "R^2 Score on Test Set: 0.4291\n"
     ]
    }
   ],
   "source": [
    "# ---- Linear Regression ----\n",
    "print(\"\\n=== Linear Regression ===\")\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_scaled, y_train_continuous)\n",
    "\n",
    "# Get coefficients and sort by importance\n",
    "linear_coefficients = linear_model.coef_\n",
    "linear_coef_df = pd.DataFrame({\n",
    "    'Feature': scalar_features,\n",
    "    'Coefficient': linear_coefficients\n",
    "}).sort_values(by='Coefficient', key=abs, ascending=False)\n",
    "\n",
    "# Print sorted coefficients\n",
    "print(\"\\nSorted Coefficients (Linear Regression):\")\n",
    "print(linear_coef_df)\n",
    "\n",
    "# Evaluate the model\n",
    "linear_r_squared = linear_model.score(X_test_scaled, y_test_continuous)\n",
    "print(f\"\\nR^2 Score on Test Set: {linear_r_squared:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processing and Flattening Embeddings**\n",
    "\n",
    "In these next few cells, we load in the embeddings and create a new train and test variable to train a decision tree model with some starting parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = df_test[[\"gene\", \"embedding\", \"blosum62_embedding\"]]\n",
    "train_embeddings = df_train[[\"gene\", \"embedding\", \"blosum62_embedding\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ESM dimensions: [1280]\n",
      "Test ESM dimensions: [1280]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87901/577380073.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_embeddings['flattened_embedding'] = train_embeddings['embedding'].apply(\n",
      "/tmp/ipykernel_87901/577380073.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_embeddings['flattened_embedding'] = test_embeddings['embedding'].apply(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Flatten ESM embeddings\n",
    "train_embeddings['flattened_embedding'] = train_embeddings['embedding'].apply(\n",
    "    lambda x: np.array(x).flatten()\n",
    ")\n",
    "test_embeddings['flattened_embedding'] = test_embeddings['embedding'].apply(\n",
    "    lambda x: np.array(x).flatten()\n",
    ")\n",
    "\n",
    "# Validate dimensions\n",
    "print(\"Train ESM dimensions:\", train_embeddings['flattened_embedding'].apply(len).unique())\n",
    "print(\"Test ESM dimensions:\", test_embeddings['flattened_embedding'].apply(len).unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape: (2019, 1280)\n",
      "Test embeddings shape: (660, 1280)\n",
      "Combined train shape: (2019, 1347)\n",
      "Combined test shape: (660, 1347)\n"
     ]
    }
   ],
   "source": [
    "X_train_embeddings = np.array(train_embeddings['flattened_embedding'].tolist())\n",
    "X_test_embeddings = np.array(test_embeddings['flattened_embedding'].tolist())\n",
    "X_train_combined = np.hstack([X_train_scaled, X_train_embeddings])\n",
    "X_test_combined = np.hstack([X_test_scaled, X_test_embeddings])\n",
    "# Check shapes\n",
    "print(\"Train embeddings shape:\", X_train_embeddings.shape)  # Should be (n_samples, 1280)\n",
    "print(\"Test embeddings shape:\", X_test_embeddings.shape)    # Should be (n_samples, 1280)\n",
    "print(\"Combined train shape:\", X_train_combined.shape)\n",
    "print(\"Combined test shape:\", X_test_combined.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost Modeling**\n",
    "\n",
    "Below we fit an XGBoost model with the features including embeddings using some manually selected parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mean Squared Error: 0.0027\n",
      "Training R^2 Score: 0.9741\n",
      "Test Mean Squared Error: 0.0526\n",
      "Test R^2 Score: 0.4914\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Train the XGBoost regressor\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,       # Number of trees\n",
    "    learning_rate=0.1,      # Learning rate\n",
    "    max_depth=5,            # Maximum depth\n",
    "    random_state=42         # For reproducibility\n",
    ")\n",
    "xgb_model.fit(X_train_combined, y_train_continuous)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = xgb_model.predict(X_train_combined)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = xgb_model.predict(X_test_combined)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_mse = mean_squared_error(y_train_continuous, y_train_pred)\n",
    "train_r2 = r2_score(y_train_continuous, y_train_pred)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_mse = mean_squared_error(y_test_continuous, y_test_pred)\n",
    "test_r2 = r2_score(y_test_continuous, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Training Mean Squared Error: {train_mse:.4f}\")\n",
    "print(f\"Training R^2 Score: {train_r2:.4f}\")\n",
    "print(f\"Test Mean Squared Error: {test_mse:.4f}\")\n",
    "print(f\"Test R^2 Score: {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning**\n",
    "\n",
    "After the above model was evaluated, we ran a 100 trial Optuna experiment for hyperparameter tuning where we also used 3-fold cross validation. I think this is actually where the confusion likely is based on our conversation at JBEI. I did actually find a mistake in our reporting which is that we reported the best hyperparameters for highest test  (trial 99), so we made a mistake in cherry picking a trial from the Optuna run. \n",
    "\n",
    "Regardless, the best trial from Optuna was number 87, which had the settings shown below. Note that the R2 is 0.437, which does not come close to our reported 0.53. This is due to the fact that training with CV will change the model performance. \n",
    "\n",
    "***The attached png in the email shows our results when doing the training with Optuna, we actually never pushed test R2 above 0.44 at this point.***\n",
    "\n",
    "However, our goal was to directly improve on the results from GATsol, which uses a defined test/train split with no CV. Their modeling strategy was to do hyperparameter tuning with CV and then report performance for the model using the single test/train with no CV. They did this because the ~15 or so models preceding this publication all used the same split to enable direct comparison, so we did this as well. \n",
    "\n",
    "[I 2024-12-14 16:17:36,056] Trial 87 finished with value: 0.43782824071107224 and parameters: {'n_estimators': 135, 'learning_rate': 0.052121761036791094, 'max_depth': 4, 'subsample': 0.8433759176013462, 'colsample_bytree': 0.869835124409304, 'min_child_weight': 2, 'gamma': 0.1032306334323324}. Best is trial 87 with value: 0.43782824071107224.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Best Model Identified by Optuna (Trial 87) ---\n",
      "Training R^2 Score (Trial 87): 0.8522\n",
      "Test R^2 Score (Trial 87): 0.5292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# --- Assume X_train_combined, y_train_continuous, X_test_combined, y_test_continuous are loaded ---\n",
    "\n",
    "# Parameters from Trial 87, which Optuna found to be the best via cross-validation\n",
    "params_trial_87 = {\n",
    "    'n_estimators': 135,\n",
    "    'learning_rate': 0.052121761036791094,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.8433759176013462,\n",
    "    'colsample_bytree': 0.869835124409304,\n",
    "    'min_child_weight': 2,\n",
    "    'gamma': 0.1032306334323324,\n",
    "    'random_state': 42  # For reproducibility\n",
    "}\n",
    "\n",
    "print(\"--- Evaluating Best Model Identified by Optuna (Trial 87) ---\")\n",
    "\n",
    "# Initialize and train the model with the best parameters\n",
    "xgb_model_best = XGBRegressor(**params_trial_87)\n",
    "xgb_model_best.fit(X_train_combined, y_train_continuous)\n",
    "\n",
    "# Evaluate the model\n",
    "y_train_pred_best = xgb_model_best.predict(X_train_combined)\n",
    "y_test_pred_best = xgb_model_best.predict(X_test_combined)\n",
    "\n",
    "train_r2_best = r2_score(y_train_continuous, y_train_pred_best)\n",
    "test_r2_best = r2_score(y_test_continuous, y_test_pred_best)\n",
    "\n",
    "print(f\"Training R^2 Score (Trial 87): {train_r2_best:.4f}\")\n",
    "print(f\"Test R^2 Score (Trial 87): {test_r2_best:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Optuna parameters performance FOR TRAIN R2 with no CV**\n",
    "\n",
    "As shown above, when we take the best parameters from our tuning run and apply this to the single split we get 0.529 (still beats GATsol, but 0.1 under what we reported)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Optuna parameters performance FOR TEST R2 with no CV**\n",
    "\n",
    "The mistake that we made in our reporting is that we identified the best model by running: best_trial = results_df.loc[results_df['r2_test'].idxmax()]\n",
    "\n",
    "This line returned the \"best\" trial with respect to test performance, not train performance. This makes the conclusion we drew incorrect because we just ended up using the parameters that optimized our objective, we should have selected on 'r2_train'.\n",
    "\n",
    "[I 2024-12-14 16:21:29,824] Trial 99 finished with value: 0.4330881580532266 and parameters: {'n_estimators': 142, 'learning_rate': 0.06235983828438135, 'max_depth': 4, 'subsample': 0.8318125531190607, 'colsample_bytree': 0.8769926991459716, 'min_child_weight': 2, 'gamma': 0.16173707656854563}. Best is trial 87 with value: 0.43782824071107224.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Best Model for Test R2 Identified by Optuna (Trial 99) ---\n",
      "Training R^2 Score (Trial 99): 0.8681\n",
      "Test R^2 Score (Trial 99): 0.5372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# --- Assume X_train_combined, y_train_continuous, X_test_combined, y_test_continuous are loaded ---\n",
    "\n",
    "params_trial_87 = {\n",
    "    'n_estimators': 142,\n",
    "    'learning_rate': 0.06235983828438135,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.8318125531190607,\n",
    "    'colsample_bytree': 0.8769926991459716,\n",
    "    'min_child_weight': 2,\n",
    "    'gamma': 0.16173707656854563,\n",
    "    'random_state': 42  # For reproducibility\n",
    "}\n",
    "\n",
    "print(\"--- Evaluating Best Model for Test R2 Identified by Optuna (Trial 99) ---\")\n",
    "\n",
    "# Initialize and train the model with the best parameters\n",
    "xgb_model_best = XGBRegressor(**params_trial_87)\n",
    "xgb_model_best.fit(X_train_combined, y_train_continuous)\n",
    "\n",
    "# Evaluate the model\n",
    "y_train_pred_best = xgb_model_best.predict(X_train_combined)\n",
    "y_test_pred_best = xgb_model_best.predict(X_test_combined)\n",
    "\n",
    "train_r2_best = r2_score(y_train_continuous, y_train_pred_best)\n",
    "test_r2_best = r2_score(y_test_continuous, y_test_pred_best)\n",
    "\n",
    "print(f\"Training R^2 Score (Trial 99): {train_r2_best:.4f}\")\n",
    "print(f\"Test R^2 Score (Trial 99): {test_r2_best:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINAL THOUGHTS**\n",
    "\n",
    "When we discussed troubleshooting on 6/18/25 you mentioned that you combined the test/train splits, ran CV, and then evaluated on a new test set. \n",
    "\n",
    "If our discussion was in reference to your evaluation on this pipeline then your 0.49 is actually beating our results when doing CV (we were getting 0.437). If you look at the GATsol paper (https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-024-05820-8), they barely pushed 0.40 with CV and only reported their 0.51 on the defined split. If you run the model on that static/defined split with no CV it might be more telling if something else is critically wrong. \n",
    "\n",
    "I imagine what is happening is that you actually improved the model by processing Blosum correctly. \n",
    "\n",
    "If I am wrong about the CV/splits then I would imagine there is a feature engineering issue that we haven't caught. But I am able to run this notebook without issues and I think I remember you saying you're using the pre-processed .pkl anyway. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PH245_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
